# Day 25 - 激活函数 & 归一化计算（小白友好版）

## 🌈 今天学习什么？
我们认识几个“神经元开关”，它们决定神经网络的输出方式。

## 🎚 常见激活函数：
- ReLU：大于 0 就放行，小于 0 关门（速度快）
- Sigmoid：像“挤牙膏”，把值压在 0-1
- Tanh：像一根绳，两边拉，压成 -1 到 1
- Softmax：像比赛打分，把一堆数变成百分比

## 🧪 归一化操作：
- BatchNorm：让每一层的输出都更“平均”，稳定训练

## ⚙️ 硬件角度
| 函数 | 需要操作 |
|------|----------|
| ReLU | 比较（快）|
| Sigmoid/Tanh | 需要除法、指数（较慢）|
| Softmax | 排序、指数、除法 |
| BN | 减法、除法、乘法 |

---

